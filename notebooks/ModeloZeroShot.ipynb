{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cffc776c",
   "metadata": {},
   "source": [
    "# Aprendizado Zero-Shot\n",
    "- Cenário onde o modelo de IA é treinado para reconhecer e categorizar conceitos ou objetos sem nunca ter visto um exemplo dessas categorias ou conceitos com antecedência\n",
    "- Ele não se refere a um algoritmo específico mas à natureza do próprio problema\n",
    "- Os problemas de aprendizado Zero-Shot fazem o uso de informações auxiliares como descrições textuais, atributos, representações incorporadas ou outras informações semânticas relevantes para a tarefa em questão\n",
    "- A transferência de aprendizado também é muito utilizada em problemas desse tipo, utilizar um modelo já treinado para uma nova tarefa pode ser útil para minimizar o tempo e recursos necessários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "313f2761",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mateus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Mateus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Reprodutibilidade\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80e6af5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemplo de dado de treino:\n",
      "{'text': 'There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier\\'s plot are far more complicated... Fortier looks more like Prime Suspect, if we have to spot similarities... The main character is weak and weirdo, but have \"clairvoyance\". People like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it\\'s the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all...', 'label': 1}\n",
      "Tamanhos -> train: 2000 | test: 200\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# 3.1 Carregar dataset IMDb\n",
    "# ============================\n",
    "dataset = load_dataset(\"stanfordnlp/imdb\")\n",
    "\n",
    "# Subconjuntos para caber no Colab\n",
    "N_TRAIN = 2000\n",
    "N_TEST  = 200\n",
    "\n",
    "train_data = dataset[\"train\"].shuffle(seed=SEED).select(range(N_TRAIN))\n",
    "test_data  = dataset[\"test\"].shuffle(seed=SEED).select(range(N_TEST))\n",
    "\n",
    "label_map = {0: \"negative\", 1: \"positive\"}\n",
    "\n",
    "print(\"Exemplo de dado de treino:\")\n",
    "print(train_data[0])\n",
    "print(\"Tamanhos -> train:\", len(train_data), \"| test:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796ce60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mateus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Mateus\\.cache\\huggingface\\hub\\models--facebook--bart-large-mnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "#Modelo bart-large-mnli é um dos modelos mais eficazes do mundo para ZERO-SHOT\n",
    "#Treinado justamente em NLI — a tarefa que permite fazer classificação sem treino\n",
    "\n",
    "# ============================\n",
    "# 3.2 Baseline: Zero-Shot com BART-MNLI\n",
    "# ============================\n",
    "zero_shot_classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"facebook/bart-large-mnli\",\n",
    "    device=0 if device == \"cuda\" else -1,\n",
    ")\n",
    "\n",
    "candidate_labels = [\"positive\", \"negative\"]\n",
    "\n",
    "def predict_zero_shot(text: str) -> str:\n",
    "    result = zero_shot_classifier(\n",
    "        text,\n",
    "        candidate_labels=candidate_labels,\n",
    "        multi_label=False,\n",
    "    )\n",
    "    # Label com maior score\n",
    "    return result[\"labels\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "000c02e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Baseline Zero-Shot (BART-MNLI): 100%|██████████| 200/200 [05:41<00:00,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Métricas Baseline: BART Zero-Shot ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8785    0.9038    0.8910       104\n",
      "    positive     0.8925    0.8646    0.8783        96\n",
      "\n",
      "    accuracy                         0.8850       200\n",
      "   macro avg     0.8855    0.8842    0.8847       200\n",
      "weighted avg     0.8852    0.8850    0.8849       200\n",
      "\n",
      "Matriz de confusão:\n",
      "[[94 10]\n",
      " [13 83]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = []\n",
    "y_pred_zero_shot = []\n",
    "\n",
    "for sample in tqdm(test_data, desc=\"Baseline Zero-Shot (BART-MNLI)\"):\n",
    "    text = sample[\"text\"]\n",
    "    true_label = label_map[sample[\"label\"]]\n",
    "    pred_label = predict_zero_shot(text)\n",
    "    y_true.append(true_label)\n",
    "    y_pred_zero_shot.append(pred_label)\n",
    "\n",
    "print(\"=== Métricas Baseline: BART Zero-Shot ===\")\n",
    "print(classification_report(y_true, y_pred_zero_shot, digits=4))\n",
    "print(\"Matriz de confusão:\")\n",
    "print(confusion_matrix(y_true, y_pred_zero_shot))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf21bfe7",
   "metadata": {},
   "source": [
    "# Por que o modelo Zero-Shot foi melhor que o RAG?\n",
    "\n",
    "- Motivo 1: RAG não é adequado para tarefas simples de classificação, sendo melhor para \n",
    "    - recuperar fatos\n",
    "    - responder perguntas\n",
    "- Motivo 2: Aumentar a base de dados pode melhorar o modelo\n",
    "- Motivo 3: A tarefa não depende de conhecimento externo, o texto já contem toda informação necessária\n",
    "- Motivo 4: O contexto aumenta o nível de “prompt noise”\n",
    "    - mais tokens inúteis o modelo precisa processar\n",
    "    - maior a chance de se desviar do foco"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
